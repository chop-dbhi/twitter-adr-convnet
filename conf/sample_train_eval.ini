# used for training on complete training set followed by evaluation on test set
[EvalConfig]
# number of times to train and evaluate, multiple runs recommended to account for random weight initialization effects
NUM_RUNS: 1

# specify the model: 0 KIM_CNNET, 1: multilayer
MODEL: 0

# Store the predicted labels for test tweets for last run
STORE_PREDICTIONS: True

# store the learned conv filters as np arrays for last run
STORE_FILTERS: True

# boosting parameters, default is No boosting
APPLY_BOOSTING: True
NUM_BOOST_ITERATIONS: 2
# should be in data/resources/boost_tweets
BOOST_TWEET_FILE: random_55000.csv

# model parameters
# these should be in the form of the SingleParameterMode examples in the sample_KIM_CNNET.ini (repeated here)
# or sample_tn_multilayer.ini
#[SingleParameterMode]
#FILTER_WIDTHS: 2, 3, 4
#USE_PRETRAINED_WORD_EMBEDDING: False
#note dropout is only used during training
#DROPOUT_KEEP_PROB: 0.65
#NUM_EPOCHS: 1
#NUM_MAPS_PER_FILTER_WIDTH: 5
#USE_STRATIFICATION: True
#WORD_EMBEDDING_LENGTH: 16

[SingleParameterMode]
FILTER_WIDTHS: 2, 3
USE_PRETRAINED_WORD_EMBEDDING: True
DROPOUT_KEEP_PROB: 0.65
NUM_EPOCHS: 1
NUM_MAPS_PER_FILTER_WIDTH: 10
USE_STRATIFICATION: True
NEGATIVE_RATIO: 0.5
WORD_EMBEDDING_FILES: word2vec_twitter_model.bin